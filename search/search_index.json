{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Documentation for the SUQ library for streamlined uncertainty quantification based on the paper:</p>  Streamlining Prediction in Bayesian Deep Learning             Rui Li, Marcus Klasson, Arno Solin, Martin Trapp International Conference on Learning Representations (ICLR 2025)"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Install the stable version with <code>pip</code>: <pre><code>pip install suq\n</code></pre></p> <p>Or install the latest development version from source: <pre><code>git clone https://github.com/AaltoML/SUQ.git\ncd SUQ\npip install -e .\n</code></pre></p>"},{"location":"#simple-usage","title":"\ud83d\ude80 Simple Usage","text":""},{"location":"#streamline-whole-network","title":"Streamline Whole Network","text":"<pre><code>from suq import streamline_mlp, streamline_vit\n\n# Load your model and estimated posterior\nmodel = ...\nposterior = ...\n\n# Wrap an MLP model with SUQ\nsuq_model = streamline_mlp(\n    model=model,\n    posterior=posterior,\n    covariance_structure='diag',       # currently only 'diag' is supported\n    likelihood='classification'        # or 'regression'\n)\n\n# Wrap a Vision Transformer with SUQ\nsuq_model = streamline_vit(\n    model=model,\n    posterior=posterior,\n    covariance_structure='diag',      # currently only 'diag' is supported\n    likelihood='classification',      \n    MLP_deterministic=True,\n    Attn_deterministic=False,\n    attention_diag_cov=False,\n    num_det_blocks=10\n)\n\n# Fit scale factor\nsuq_model.fit(train_loader, scale_fit_epoch, scale_fit_lr)\n\n# Make a prediction\npred = suq_model(X)\n</code></pre> <p>\ud83d\udcc4 See <code>examples/mlp_la_example.py</code>, <code>examples/vit_la_example.py</code>, <code>examples/mlp_vi_example.py</code>, and <code>examples/vit_vi_example.py</code> for full, self-contained examples that cover: - Training the MAP model - Estimating the posterior with Laplace or IVON (mean field VI) - Wrapping the model into a streamlined SUQ version</p> <p>\u2757 Note on Vision Transformer Support Currently, SUQ only supports Vision Transformers implemented in the same style as <code>examples/vit_model.py</code>. If you're using a different ViT implementation, compatibility is not guaranteed.</p>"},{"location":"#streamline-individual-layers","title":"Streamline Individual Layers","text":"<p>In addition to wrapping full models like MLPs or ViTs, SUQ allows you to manually wrap individual layers in your own networks.</p> <p>You can directly import supported modules from <code>suq.streamline_layer</code>.</p> <p>Supported Layers:</p> Layer Type SUQ Wrapper <code>nn.Linear</code> <code>SUQ_Linear_Diag</code> <code>nn.ReLU</code>, etc. <code>SUQ_Activation_Diag</code> <code>nn.BatchNorm1d</code> <code>SUQ_BatchNorm_Diag</code> <code>nn.LayerNorm</code> <code>SUQ_LayerNorm_Diag</code> <code>MLP (Transformer block)</code> <code>SUQ_TransformerMLP_Diag</code> <code>Attention</code> <code>SUQ_Attention_Diag</code> <code>Transformer block</code> <code>SUQ_Transformer_Block_Diag</code> <code>Final classifier</code> <code>SUQ_Classifier_Diag</code> <p>Example:</p> <pre><code>from suq.streamline_layer import SUQ_Linear_Diag\n\n# Define a standard linear layer\nlinear_layer = nn.Linear(100, 50)\n# Provide posterior variances for weights and biases\nw_var = torch.rand(50, 100)\nb_var = torch.rand(50)\n\n# Wrap the layer with SUQ's linear module\nstreamlined_layer = SUQ_Linear_Diag(linear_layer, w_var, b_var)\n\n# Provide input mean and variance (e.g., from a previous layer)\ninput_mean = torch.randn(32, 100)\ninput_var = torch.rand(32, 100)\n\n# Forward pass through the streamlined layer\npred_mean, pred_var = streamlined_layer(input_mean, input_var)\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you use this library, please cite the following publication: <pre><code>@inproceedings{li2025streamlining,\n  title = {Streamlining Prediction in {Bayesian} Deep Learning},\n  author = {Rui Li, Marcus Klasson, Arno Solin and Martin Trapp},\n  booktitle = {International Conference on Learning Representations ({ICLR})},\n  year = {2025}\n}\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>This software is provided under the MIT license.</p>"},{"location":"reference/","title":"SUQ Documentation","text":""},{"location":"reference/#base-functions","title":"Base Functions","text":""},{"location":"reference/#suq.base_suq-classes","title":"Classes","text":""},{"location":"reference/#suq.base_suq.SUQ_Base","title":"<code>SUQ_Base</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for SUQ models.</p> <p>Provides core functionality for:</p> <ul> <li>Managing likelihood type (regression or classification)</li> <li>Probit-based approximation for classification</li> <li>NLPD-based fitting of the scale factor</li> </ul> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>str</code> <p>Either <code>classification</code> or <code>regression</code>.</p> required <code>scale_init</code> <code>float</code> <p>Initial value for the scale factor parameter.</p> required Source code in <code>suq/base_suq.py</code> <pre><code>class SUQ_Base(nn.Module):\n    \"\"\"\n    Base class for SUQ models.\n\n    Provides core functionality for:\n\n    - Managing likelihood type (regression or classification)\n    - Probit-based approximation for classification\n    - NLPD-based fitting of the scale factor\n\n    Args:\n        likelihood (str): Either `classification` or `regression`.\n        scale_init (float): Initial value for the scale factor parameter.\n    \"\"\"\n\n    def __init__(self, likelihood, scale_init):\n        super().__init__()\n\n        if likelihood not in ['classification', 'regression']:\n            raise ValueError(f\"Invalid likelihood type {likelihood}\")\n\n        self.likelihood = likelihood\n        self.scale_factor = nn.Parameter(torch.Tensor([scale_init]).to(device))\n\n    def probit_approximation(self, out_mean, out_var):\n        \"\"\"\n        Applies a probit approximation to compute class probabilities from the latent Gaussian distribution.\n\n        Args:\n            out_mean (Tensor): Latent function mean, shape [B, num_classes]\n            out_var (Tensor): Latent function variance, shape [B, num_classes] or [B, num_classes, num_classes]\n\n        Returns:\n            posterior_predict_mean (Tensor): Predicted class probabilities, shape [B, num_classes]\n        \"\"\"\n\n        if out_var.dim() == 3:\n            kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var.diagonal(dim1=1, dim2=2))\n        else:\n            kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var)\n\n        posterior_predict_mean = torch.softmax(kappa * out_mean, dim=-1)\n        return posterior_predict_mean\n\n    def fit_scale_factor(self, data_loader, n_epoches, lr, speedup = True, verbose = False):\n        \"\"\"\n        Fits the scale factor for predictive variance using negative log predictive density (NLPD).\n\n        Args:\n            data_loader (DataLoader): Dataloader containing (input, target) pairs\n            n_epoches (int): Number of epochs for optimization\n            lr (float): Learning rate for scale optimizer\n            speedup (bool): If True (classification only), caches forward pass outputs to accelerate fitting\n            verbose (bool): If True, prints NLPD at each epoch\n\n        Returns:\n            total_train_nlpd (List[float]): Average NLPD per epoch over training data\n        \"\"\"\n        print(\"fit scale factor\")\n        optimizer = torch.optim.Adam([self.scale_factor], lr)\n        total_train_nlpd = []\n\n        # store intermediate result and pack it into a data loader, so we only need to do one forward pass\n        if speedup:\n\n            if self.likelihood == 'regression':\n                raise ValueError(f\"Speed up not supported for regression atm\")\n\n            if self.likelihood == 'classification':\n\n                f_mean = []\n                f_var = []\n                labels = []\n\n                for (X, y) in tqdm(data_loader, desc= \"packing f_mean f_var into a dataloader\"):\n                    out_mean, out_var = self.forward_latent(X.to(device))\n                    f_mean.append(out_mean.detach().cpu().numpy())\n                    f_var.append(out_var.detach().cpu().numpy())\n                    if y.dim() == 2:\n                        labels.append(y.numpy().argmax(1).reshape(-1, 1))\n                    if y.dim() == 1:\n                        labels.append(y.numpy().reshape(-1, 1))\n\n                f_mean = np.vstack(f_mean)\n                f_var = np.vstack(f_var)\n                labels = np.vstack(labels)\n\n                scale_fit_dataset = torch_dataset(f_mean, f_var, labels)\n                scale_fit_dataloader = DataLoader(scale_fit_dataset, batch_size=16, shuffle=True)\n\n                for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                    running_nlpd = 0\n                    for data_pair in scale_fit_dataloader:\n                        x_mean, x_var_label = data_pair\n                        num_class = x_mean.shape[1]\n                        x_mean = x_mean.to(device)\n                        x_var, label = x_var_label.split(num_class, dim=1)\n                        x_var = x_var.to(device)\n                        label = label.to(device)\n\n                        optimizer.zero_grad()\n                        # make prediction\n                        x_var = x_var / self.scale_factor.to(device)\n                        posterior_predict_mean = self.probit_approximation(x_mean, x_var)\n                        # construct log posterior predictive distribution\n                        posterior_predictive_dist = Categorical(posterior_predict_mean)\n                        # calculate nlpd and update\n                        nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                        nlpd.backward()\n                        optimizer.step()\n                        # log nlpd\n                        running_nlpd += nlpd.item()\n                    total_train_nlpd.append(running_nlpd / len(scale_fit_dataloader))\n                    if verbose:\n                        print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n                del scale_fit_dataloader\n                del scale_fit_dataset\n\n        else:\n\n            if self.likelihood == 'classification':\n                for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                    running_nlpd = 0\n                    for (data, label) in data_loader:\n\n                        data = data.to(device)\n                        label = label.to(device)\n\n                        optimizer.zero_grad()\n                        # make prediction\n                        posterior_predict_mean = self.forward(data)\n                        # construct log posterior predictive distribution\n                        posterior_predictive_dist = Categorical(posterior_predict_mean)\n                        # calculate nlpd and update\n                        nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                        nlpd.backward()\n                        optimizer.step()\n                        # log nlpd\n                        running_nlpd += nlpd.item()\n                    total_train_nlpd.append(running_nlpd / len(data_loader))\n                    if verbose:\n                        print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n\n            if self.likelihood == 'regression':\n                for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                    running_nlpd = 0\n                    for (data, label) in data_loader:\n                        data = data.to(device)\n                        label = label.to(device)\n\n                        optimizer.zero_grad()\n                        # make prediction\n                        posterior_predict_mean, posterior_predict_var = self.forward(data)\n                        # construct log posterior predictive distribution\n                        posterior_predictive_dist = Normal(posterior_predict_mean, posterior_predict_var.sqrt())\n                        # calculate nlpd and update\n                        nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                        nlpd.backward()\n                        optimizer.step()\n                        # log nlpd\n                        running_nlpd += nlpd.item()\n\n                    total_train_nlpd.append(running_nlpd / len(data_loader))\n\n                    if verbose:\n                        print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n        return total_train_nlpd\n</code></pre>"},{"location":"reference/#suq.base_suq.SUQ_Base-functions","title":"Functions","text":""},{"location":"reference/#suq.base_suq.SUQ_Base.probit_approximation","title":"<code>probit_approximation(out_mean, out_var)</code>","text":"<p>Applies a probit approximation to compute class probabilities from the latent Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>out_mean</code> <code>Tensor</code> <p>Latent function mean, shape [B, num_classes]</p> required <code>out_var</code> <code>Tensor</code> <p>Latent function variance, shape [B, num_classes] or [B, num_classes, num_classes]</p> required <p>Returns:</p> Name Type Description <code>posterior_predict_mean</code> <code>Tensor</code> <p>Predicted class probabilities, shape [B, num_classes]</p> Source code in <code>suq/base_suq.py</code> <pre><code>def probit_approximation(self, out_mean, out_var):\n    \"\"\"\n    Applies a probit approximation to compute class probabilities from the latent Gaussian distribution.\n\n    Args:\n        out_mean (Tensor): Latent function mean, shape [B, num_classes]\n        out_var (Tensor): Latent function variance, shape [B, num_classes] or [B, num_classes, num_classes]\n\n    Returns:\n        posterior_predict_mean (Tensor): Predicted class probabilities, shape [B, num_classes]\n    \"\"\"\n\n    if out_var.dim() == 3:\n        kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var.diagonal(dim1=1, dim2=2))\n    else:\n        kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var)\n\n    posterior_predict_mean = torch.softmax(kappa * out_mean, dim=-1)\n    return posterior_predict_mean\n</code></pre>"},{"location":"reference/#suq.base_suq.SUQ_Base.fit_scale_factor","title":"<code>fit_scale_factor(data_loader, n_epoches, lr, speedup=True, verbose=False)</code>","text":"<p>Fits the scale factor for predictive variance using negative log predictive density (NLPD).</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>Dataloader containing (input, target) pairs</p> required <code>n_epoches</code> <code>int</code> <p>Number of epochs for optimization</p> required <code>lr</code> <code>float</code> <p>Learning rate for scale optimizer</p> required <code>speedup</code> <code>bool</code> <p>If True (classification only), caches forward pass outputs to accelerate fitting</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints NLPD at each epoch</p> <code>False</code> <p>Returns:</p> Name Type Description <code>total_train_nlpd</code> <code>List[float]</code> <p>Average NLPD per epoch over training data</p> Source code in <code>suq/base_suq.py</code> <pre><code>def fit_scale_factor(self, data_loader, n_epoches, lr, speedup = True, verbose = False):\n    \"\"\"\n    Fits the scale factor for predictive variance using negative log predictive density (NLPD).\n\n    Args:\n        data_loader (DataLoader): Dataloader containing (input, target) pairs\n        n_epoches (int): Number of epochs for optimization\n        lr (float): Learning rate for scale optimizer\n        speedup (bool): If True (classification only), caches forward pass outputs to accelerate fitting\n        verbose (bool): If True, prints NLPD at each epoch\n\n    Returns:\n        total_train_nlpd (List[float]): Average NLPD per epoch over training data\n    \"\"\"\n    print(\"fit scale factor\")\n    optimizer = torch.optim.Adam([self.scale_factor], lr)\n    total_train_nlpd = []\n\n    # store intermediate result and pack it into a data loader, so we only need to do one forward pass\n    if speedup:\n\n        if self.likelihood == 'regression':\n            raise ValueError(f\"Speed up not supported for regression atm\")\n\n        if self.likelihood == 'classification':\n\n            f_mean = []\n            f_var = []\n            labels = []\n\n            for (X, y) in tqdm(data_loader, desc= \"packing f_mean f_var into a dataloader\"):\n                out_mean, out_var = self.forward_latent(X.to(device))\n                f_mean.append(out_mean.detach().cpu().numpy())\n                f_var.append(out_var.detach().cpu().numpy())\n                if y.dim() == 2:\n                    labels.append(y.numpy().argmax(1).reshape(-1, 1))\n                if y.dim() == 1:\n                    labels.append(y.numpy().reshape(-1, 1))\n\n            f_mean = np.vstack(f_mean)\n            f_var = np.vstack(f_var)\n            labels = np.vstack(labels)\n\n            scale_fit_dataset = torch_dataset(f_mean, f_var, labels)\n            scale_fit_dataloader = DataLoader(scale_fit_dataset, batch_size=16, shuffle=True)\n\n            for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                running_nlpd = 0\n                for data_pair in scale_fit_dataloader:\n                    x_mean, x_var_label = data_pair\n                    num_class = x_mean.shape[1]\n                    x_mean = x_mean.to(device)\n                    x_var, label = x_var_label.split(num_class, dim=1)\n                    x_var = x_var.to(device)\n                    label = label.to(device)\n\n                    optimizer.zero_grad()\n                    # make prediction\n                    x_var = x_var / self.scale_factor.to(device)\n                    posterior_predict_mean = self.probit_approximation(x_mean, x_var)\n                    # construct log posterior predictive distribution\n                    posterior_predictive_dist = Categorical(posterior_predict_mean)\n                    # calculate nlpd and update\n                    nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                    nlpd.backward()\n                    optimizer.step()\n                    # log nlpd\n                    running_nlpd += nlpd.item()\n                total_train_nlpd.append(running_nlpd / len(scale_fit_dataloader))\n                if verbose:\n                    print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n            del scale_fit_dataloader\n            del scale_fit_dataset\n\n    else:\n\n        if self.likelihood == 'classification':\n            for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                running_nlpd = 0\n                for (data, label) in data_loader:\n\n                    data = data.to(device)\n                    label = label.to(device)\n\n                    optimizer.zero_grad()\n                    # make prediction\n                    posterior_predict_mean = self.forward(data)\n                    # construct log posterior predictive distribution\n                    posterior_predictive_dist = Categorical(posterior_predict_mean)\n                    # calculate nlpd and update\n                    nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                    nlpd.backward()\n                    optimizer.step()\n                    # log nlpd\n                    running_nlpd += nlpd.item()\n                total_train_nlpd.append(running_nlpd / len(data_loader))\n                if verbose:\n                    print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n\n        if self.likelihood == 'regression':\n            for epoch in tqdm(range(n_epoches), desc=\"fitting scaling factor\"):\n                running_nlpd = 0\n                for (data, label) in data_loader:\n                    data = data.to(device)\n                    label = label.to(device)\n\n                    optimizer.zero_grad()\n                    # make prediction\n                    posterior_predict_mean, posterior_predict_var = self.forward(data)\n                    # construct log posterior predictive distribution\n                    posterior_predictive_dist = Normal(posterior_predict_mean, posterior_predict_var.sqrt())\n                    # calculate nlpd and update\n                    nlpd = -posterior_predictive_dist.log_prob(label).mean()\n                    nlpd.backward()\n                    optimizer.step()\n                    # log nlpd\n                    running_nlpd += nlpd.item()\n\n                total_train_nlpd.append(running_nlpd / len(data_loader))\n\n                if verbose:\n                    print(f\"epoch {epoch + 1}, nlpd {total_train_nlpd[-1]}\")\n\n    return total_train_nlpd\n</code></pre>"},{"location":"reference/#mlp-functions-with-diagonal-covariance","title":"MLP Functions with Diagonal Covariance","text":""},{"location":"reference/#suq.diag_suq_mlp-classes","title":"Classes","text":""},{"location":"reference/#suq.diag_suq_mlp.SUQ_Linear_Diag","title":"<code>SUQ_Linear_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>Linear layer with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps a standard <code>nn.Linear</code> layer and applies closed-form mean and variance propagation. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>org_linear</code> <code>Linear</code> <p>The original linear layer to wrap      </p> required <code>w_var</code> <code>Tensor</code> <p>Element-wise variance of the weights <code>W</code>. Shape: <code>[D_out, D_in]</code></p> required <code>b_var</code> <code>Tensor</code> <p>Element-wise variance of the bias <code>b</code>. Shape: <code>[D_out, ]</code></p> required Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>class SUQ_Linear_Diag(nn.Module):\n    \"\"\"\n    Linear layer with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps a standard `nn.Linear` layer and applies closed-form mean and variance propagation. See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        org_linear (nn.Linear): The original linear layer to wrap      \n        w_var (Tensor): Element-wise variance of the weights `W`. Shape: `[D_out, D_in]`\n        b_var (Tensor): Element-wise variance of the bias `b`. Shape: `[D_out, ]`\n    \"\"\"\n    def __init__(self, org_linear, w_var, b_var):\n        super().__init__()\n\n        self.weight = org_linear.weight.data\n        self.bias = org_linear.bias.data\n        self.w_var = w_var\n        self.b_var = b_var\n\n    def forward(self, a_mean, a_var): \n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ linear layer.\n\n        Args:\n            a_mean (Tensor): Input mean. Shape: `[B, D_in]`\n            a_var (Tensor): Input element-wise variance. Shape: `[B, D_in]`\n\n        Returns:\n            h_mean (Tensor): Mean of the output `h'. Shape: `[B, D_out]`\n            h_var (Tensor): Element-wise variance of output `h'. Shape: `[B, D_out]`\n        \"\"\"\n\n        if a_var == None:\n            a_var = torch.zeros_like(a_mean).to(a_mean.device)\n\n        h_mean, h_var = forward_aW_diag(a_mean, a_var, self.weight, self.bias, self.w_var, self.b_var)\n\n        return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_Linear_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_mlp.SUQ_Linear_Diag.forward","title":"<code>forward(a_mean, a_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>a_mean</code> <code>Tensor</code> <p>Input mean. Shape: <code>[B, D_in]</code></p> required <code>a_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape: <code>[B, D_in]</code></p> required <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Mean of the output <code>h'. Shape:</code>[B, D_out]`</p> <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of output <code>h'. Shape:</code>[B, D_out]`</p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward(self, a_mean, a_var): \n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ linear layer.\n\n    Args:\n        a_mean (Tensor): Input mean. Shape: `[B, D_in]`\n        a_var (Tensor): Input element-wise variance. Shape: `[B, D_in]`\n\n    Returns:\n        h_mean (Tensor): Mean of the output `h'. Shape: `[B, D_out]`\n        h_var (Tensor): Element-wise variance of output `h'. Shape: `[B, D_out]`\n    \"\"\"\n\n    if a_var == None:\n        a_var = torch.zeros_like(a_mean).to(a_mean.device)\n\n    h_mean, h_var = forward_aW_diag(a_mean, a_var, self.weight, self.bias, self.w_var, self.b_var)\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_Activation_Diag","title":"<code>SUQ_Activation_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>Activation layer with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps a standard activation function and applies a first-order approximation to propagate input variance through the nonlinearity. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>afun</code> <code>Callable</code> <p>A PyTorch activation function (e.g. <code>nn.ReLU()</code>)</p> required Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>class SUQ_Activation_Diag(nn.Module):\n    \"\"\"\n    Activation layer with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps a standard activation function and applies a first-order approximation to propagate input variance through the nonlinearity. See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        afun (Callable): A PyTorch activation function (e.g. `nn.ReLU()`)\n    \"\"\"\n\n    def __init__(self, afun):        \n        super().__init__()\n        self.afun = afun\n\n    def forward(self, h_mean, h_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ activation layer.\n\n        Args:\n            h_mean (Tensor): Mean of the pre-activations `h`. Shape: `[B, D]`\n            h_var (Tensor): Element-wise variance of the pre-activation `h`. Shape: `[B, D]`\n\n        Returns:\n            a_mean (Tensor): Mean of the activation `a`. Shape: [B, D]\n            a_var (Tensor): Element-wise variance of the activation `a`. Shape: `[B, D]`\n        \"\"\"\n        a_mean, a_var = forward_activation_implicit_diag(self.afun, h_mean, h_var)\n        return a_mean, a_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_Activation_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_mlp.SUQ_Activation_Diag.forward","title":"<code>forward(h_mean, h_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ activation layer.</p> <p>Parameters:</p> Name Type Description Default <code>h_mean</code> <code>Tensor</code> <p>Mean of the pre-activations <code>h</code>. Shape: <code>[B, D]</code></p> required <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the pre-activation <code>h</code>. Shape: <code>[B, D]</code></p> required <p>Returns:</p> Name Type Description <code>a_mean</code> <code>Tensor</code> <p>Mean of the activation <code>a</code>. Shape: [B, D]</p> <code>a_var</code> <code>Tensor</code> <p>Element-wise variance of the activation <code>a</code>. Shape: <code>[B, D]</code></p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward(self, h_mean, h_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ activation layer.\n\n    Args:\n        h_mean (Tensor): Mean of the pre-activations `h`. Shape: `[B, D]`\n        h_var (Tensor): Element-wise variance of the pre-activation `h`. Shape: `[B, D]`\n\n    Returns:\n        a_mean (Tensor): Mean of the activation `a`. Shape: [B, D]\n        a_var (Tensor): Element-wise variance of the activation `a`. Shape: `[B, D]`\n    \"\"\"\n    a_mean, a_var = forward_activation_implicit_diag(self.afun, h_mean, h_var)\n    return a_mean, a_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_BatchNorm_Diag","title":"<code>SUQ_BatchNorm_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>BatchNorm layer with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps <code>nn.BatchNorm1d</code> and adjusts input variance using batch normalization statistics and scale parameters. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>BatchNorm</code> <code>BatchNorm1d</code> <p>The original batch norm layer</p> required Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>class SUQ_BatchNorm_Diag(nn.Module):\n    \"\"\"\n    BatchNorm layer with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps `nn.BatchNorm1d` and adjusts input variance using batch normalization statistics and scale parameters. See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        BatchNorm (nn.BatchNorm1d): The original batch norm layer\n    \"\"\"\n\n    def __init__(self, BatchNorm):\n        super().__init__()\n\n        self.BatchNorm = BatchNorm\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ BatchNorm layer.\n\n        Args:\n            x_mean (Tensor): Input mean. Shape: [B, D]\n            x_var (Tensor): Input element-wise variance. Shape: [B, D]\n\n        Returns:\n            out_mean (Tensor): Output mean after batch normalization. Shape: [B, D]\n            out_var (Tensor): Output element-wise variance after batch normalization. Shape: [B, D]\n        \"\"\"\n\n        with torch.no_grad():\n\n            out_mean = self.BatchNorm.forward(x_mean)\n            out_var = forward_batch_norm_diag(x_mean, x_var, self.BatchNorm.weight, 1e-5)\n\n        return out_mean, out_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_BatchNorm_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_mlp.SUQ_BatchNorm_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ BatchNorm layer.</p> <p>Parameters:</p> Name Type Description Default <code>x_mean</code> <code>Tensor</code> <p>Input mean. Shape: [B, D]</p> required <code>x_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape: [B, D]</p> required <p>Returns:</p> Name Type Description <code>out_mean</code> <code>Tensor</code> <p>Output mean after batch normalization. Shape: [B, D]</p> <code>out_var</code> <code>Tensor</code> <p>Output element-wise variance after batch normalization. Shape: [B, D]</p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ BatchNorm layer.\n\n    Args:\n        x_mean (Tensor): Input mean. Shape: [B, D]\n        x_var (Tensor): Input element-wise variance. Shape: [B, D]\n\n    Returns:\n        out_mean (Tensor): Output mean after batch normalization. Shape: [B, D]\n        out_var (Tensor): Output element-wise variance after batch normalization. Shape: [B, D]\n    \"\"\"\n\n    with torch.no_grad():\n\n        out_mean = self.BatchNorm.forward(x_mean)\n        out_var = forward_batch_norm_diag(x_mean, x_var, self.BatchNorm.weight, 1e-5)\n\n    return out_mean, out_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_MLP_Diag","title":"<code>SUQ_MLP_Diag</code>","text":"<p>               Bases: <code>SUQ_Base</code></p> <p>Multilayer perceptron model with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps a standard MLP, converting its layers into SUQ-compatible components. Supports both classification and regression via predictive Gaussian approximation.</p> Note <p>The input model should correspond to the latent function only: - For regression, this is the full model (including final output layer). - For classification, exclude the softmax layer and pass only the logit-producing part.</p> <p>Parameters:</p> Name Type Description Default <code>org_model</code> <code>Module</code> <p>The original MLP model to convert</p> required <code>posterior_variance</code> <code>Tensor</code> <p>Flattened posterior variance vector</p> required <code>likelihood</code> <code>str</code> <p>Either 'classification' or 'regression'</p> required <code>scale_init</code> <code>float</code> <p>Initial scale factor</p> <code>1.0</code> <code>sigma_noise</code> <code>float</code> <p>noise level (for regression)</p> <code>None</code> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>class SUQ_MLP_Diag(SUQ_Base):\n    \"\"\"\n    Multilayer perceptron model with closed-form uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps a standard MLP, converting its layers into SUQ-compatible components.\n    Supports both classification and regression via predictive Gaussian approximation.\n\n    Note:\n        The input model should correspond to the latent function only:\n        - For regression, this is the full model (including final output layer).\n        - For classification, exclude the softmax layer and pass only the logit-producing part.\n\n    Args:\n        org_model (nn.Module): The original MLP model to convert\n        posterior_variance (Tensor): Flattened posterior variance vector\n        likelihood (str): Either 'classification' or 'regression'\n        scale_init (float, optional): Initial scale factor\n        sigma_noise (float, optional): noise level (for regression)\n    \"\"\"\n\n    def __init__(self, org_model, posterior_variance, likelihood, scale_init = 1.0, sigma_noise = None):\n        super().__init__(likelihood, scale_init)\n\n        self.sigma_noise = sigma_noise\n        self.convert_model(org_model, posterior_variance)\n\n    def forward_latent(self, data, out_var = None):\n        \"\"\"\n        Compute the predictive mean and variance of the latent function before applying the likelihood.\n\n        Traverses the model layer by layer, propagating mean and variance through each SUQ-wrapped layer.\n\n        Args:\n            data (Tensor): Input data. Shape: [B, D_in]\n            out_var (Tensor or None): Optional input variance. Shape: [B, D_in]\n\n        Returns:\n            out_mean (Tensor): Output mean after final layer. Shape: [B, D_out]\n            out_var (Tensor): Output element-wise variance after final layer. Shape: [B, D_out]\n        \"\"\"\n\n        out_mean = data\n\n        if isinstance(self.model, nn.Sequential):\n            for layer in self.model:\n                out_mean, out_var = layer.forward(out_mean, out_var)\n        ##TODO: other type of model            \n\n        out_var = out_var / self.scale_factor\n\n        return out_mean, out_var\n\n    def forward(self, data):\n        \"\"\"\n        Compute the predictive distribution based on the model's likelihood setting.\n\n        For classification, use probit-approximation.\n        For regression, returns the latent mean and total predictive variance.\n\n        Args:\n            data (Tensor): Input data. Shape: [B, D]\n\n        Returns:\n            If classification:\n                Tensor: Class probabilities. Shape: [B, num_classes]\n            If regression:\n                Tuple[Tensor, Tensor]: Output mean and element-wise variance. Shape: [B, D_out]\n        \"\"\"\n\n        out_mean, out_var = self.forward_latent(data)\n\n        if self.likelihood == 'classification':\n            kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var)\n            return torch.softmax(kappa * out_mean, dim=-1)\n\n        if self.likelihood == 'regression':\n            return out_mean, out_var + self.sigma_noise ** 2\n\n    def convert_model(self, org_model, posterior_variance):\n        \"\"\"\n        Converts a deterministic MLP into a SUQ-compatible model with diagonal posterior.\n\n        Each layer is replaced with its corresponding SUQ module (e.g. linear, activation, batchnorm), using the provided flattened posterior variance vector.\n\n        Args:\n            org_model (nn.Module): The original model to convert (latent function only)\n            posterior_variance (Tensor): Flattened posterior variance for Bayesian parameters\n        \"\"\"\n\n        p_model = copy.deepcopy(org_model)\n\n        loc = 0\n        for n, layer in p_model.named_modules():\n            if isinstance(layer, nn.Linear):\n\n                D_out, D_in = layer.weight.data.shape\n                num_param = torch.numel(parameters_to_vector(layer.parameters()))\n                num_weight_param = D_out * D_in\n\n                covariance_block = posterior_variance[loc : loc + num_param]\n\n                b_var = torch.zeros_like(layer.bias.data).to(layer.bias.data.device)\n                w_var = torch.zeros_like(layer.weight.data).to(layer.bias.data.device)\n\n                for k in range(D_out):\n                    b_var[k] = covariance_block[num_weight_param + k]\n                    for i in range(D_in):\n                        w_var[k][i] = covariance_block[k * D_in + i]\n\n                new_layer = SUQ_Linear_Diag(layer, w_var, b_var)\n\n                loc += num_param\n                setattr(p_model, n, new_layer)\n\n            if isinstance(layer, nn.BatchNorm1d):\n                new_layer = SUQ_BatchNorm_Diag(layer)\n                setattr(p_model, n, new_layer)\n\n            if type(layer).__name__ in torch.nn.modules.activation.__all__:\n                new_layer = SUQ_Activation_Diag(layer)\n                setattr(p_model, n, new_layer)\n\n        self.model = p_model\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_MLP_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_mlp.SUQ_MLP_Diag.forward_latent","title":"<code>forward_latent(data, out_var=None)</code>","text":"<p>Compute the predictive mean and variance of the latent function before applying the likelihood.</p> <p>Traverses the model layer by layer, propagating mean and variance through each SUQ-wrapped layer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input data. Shape: [B, D_in]</p> required <code>out_var</code> <code>Tensor or None</code> <p>Optional input variance. Shape: [B, D_in]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_mean</code> <code>Tensor</code> <p>Output mean after final layer. Shape: [B, D_out]</p> <code>out_var</code> <code>Tensor</code> <p>Output element-wise variance after final layer. Shape: [B, D_out]</p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward_latent(self, data, out_var = None):\n    \"\"\"\n    Compute the predictive mean and variance of the latent function before applying the likelihood.\n\n    Traverses the model layer by layer, propagating mean and variance through each SUQ-wrapped layer.\n\n    Args:\n        data (Tensor): Input data. Shape: [B, D_in]\n        out_var (Tensor or None): Optional input variance. Shape: [B, D_in]\n\n    Returns:\n        out_mean (Tensor): Output mean after final layer. Shape: [B, D_out]\n        out_var (Tensor): Output element-wise variance after final layer. Shape: [B, D_out]\n    \"\"\"\n\n    out_mean = data\n\n    if isinstance(self.model, nn.Sequential):\n        for layer in self.model:\n            out_mean, out_var = layer.forward(out_mean, out_var)\n    ##TODO: other type of model            \n\n    out_var = out_var / self.scale_factor\n\n    return out_mean, out_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_MLP_Diag.forward","title":"<code>forward(data)</code>","text":"<p>Compute the predictive distribution based on the model's likelihood setting.</p> <p>For classification, use probit-approximation. For regression, returns the latent mean and total predictive variance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input data. Shape: [B, D]</p> required <p>Returns:</p> Type Description <p>If classification: Tensor: Class probabilities. Shape: [B, num_classes]</p> <p>If regression: Tuple[Tensor, Tensor]: Output mean and element-wise variance. Shape: [B, D_out]</p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward(self, data):\n    \"\"\"\n    Compute the predictive distribution based on the model's likelihood setting.\n\n    For classification, use probit-approximation.\n    For regression, returns the latent mean and total predictive variance.\n\n    Args:\n        data (Tensor): Input data. Shape: [B, D]\n\n    Returns:\n        If classification:\n            Tensor: Class probabilities. Shape: [B, num_classes]\n        If regression:\n            Tuple[Tensor, Tensor]: Output mean and element-wise variance. Shape: [B, D_out]\n    \"\"\"\n\n    out_mean, out_var = self.forward_latent(data)\n\n    if self.likelihood == 'classification':\n        kappa = 1 / torch.sqrt(1. + np.pi / 8 * out_var)\n        return torch.softmax(kappa * out_mean, dim=-1)\n\n    if self.likelihood == 'regression':\n        return out_mean, out_var + self.sigma_noise ** 2\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.SUQ_MLP_Diag.convert_model","title":"<code>convert_model(org_model, posterior_variance)</code>","text":"<p>Converts a deterministic MLP into a SUQ-compatible model with diagonal posterior.</p> <p>Each layer is replaced with its corresponding SUQ module (e.g. linear, activation, batchnorm), using the provided flattened posterior variance vector.</p> <p>Parameters:</p> Name Type Description Default <code>org_model</code> <code>Module</code> <p>The original model to convert (latent function only)</p> required <code>posterior_variance</code> <code>Tensor</code> <p>Flattened posterior variance for Bayesian parameters</p> required Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def convert_model(self, org_model, posterior_variance):\n    \"\"\"\n    Converts a deterministic MLP into a SUQ-compatible model with diagonal posterior.\n\n    Each layer is replaced with its corresponding SUQ module (e.g. linear, activation, batchnorm), using the provided flattened posterior variance vector.\n\n    Args:\n        org_model (nn.Module): The original model to convert (latent function only)\n        posterior_variance (Tensor): Flattened posterior variance for Bayesian parameters\n    \"\"\"\n\n    p_model = copy.deepcopy(org_model)\n\n    loc = 0\n    for n, layer in p_model.named_modules():\n        if isinstance(layer, nn.Linear):\n\n            D_out, D_in = layer.weight.data.shape\n            num_param = torch.numel(parameters_to_vector(layer.parameters()))\n            num_weight_param = D_out * D_in\n\n            covariance_block = posterior_variance[loc : loc + num_param]\n\n            b_var = torch.zeros_like(layer.bias.data).to(layer.bias.data.device)\n            w_var = torch.zeros_like(layer.weight.data).to(layer.bias.data.device)\n\n            for k in range(D_out):\n                b_var[k] = covariance_block[num_weight_param + k]\n                for i in range(D_in):\n                    w_var[k][i] = covariance_block[k * D_in + i]\n\n            new_layer = SUQ_Linear_Diag(layer, w_var, b_var)\n\n            loc += num_param\n            setattr(p_model, n, new_layer)\n\n        if isinstance(layer, nn.BatchNorm1d):\n            new_layer = SUQ_BatchNorm_Diag(layer)\n            setattr(p_model, n, new_layer)\n\n        if type(layer).__name__ in torch.nn.modules.activation.__all__:\n            new_layer = SUQ_Activation_Diag(layer)\n            setattr(p_model, n, new_layer)\n\n    self.model = p_model\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_mlp.forward_aW_diag","title":"<code>forward_aW_diag(a_mean, a_var, weight, bias, w_var, b_var)</code>","text":"<p>Compute the mean and element-wise variance of <code>h = a @ W^T + b</code> when the posterior has diagonal covariance.</p> <p>Parameters:</p> Name Type Description Default <code>a_mean</code> <code>Tensor</code> <p>Mean of the input <code>a</code>. Shape: <code>[B, D_in]</code>.</p> required <code>a_var</code> <code>Tensor</code> <p>Variance of the input <code>a</code>. Shape: <code>[B, D_in]</code></p> required <code>weight</code> <code>Tensor</code> <p>Mean of the weights <code>W</code>. Shape: <code>[D_out, D_in]</code></p> required <code>bias</code> <code>Tensor</code> <p>Mean of the bias <code>b</code>. Shape: <code>[D_out, ]</code></p> required <code>b_var</code> <code>Tensor</code> <p>Element-wise variance of the bias <code>b</code>. Shape: <code>[D_out, ]</code></p> required <code>w_var</code> <code>Tensor</code> <p>Element-wise variance of the weights <code>W</code>. Shape: <code>[D_out, D_in]</code></p> required <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Mean of the pre-activations <code>h</code>. Shape: <code>[B, D_out]</code></p> <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the pre-activations <code>h</code>. Shape: <code>[B, D_out]</code></p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward_aW_diag(a_mean, a_var, weight, bias, w_var, b_var):\n    \"\"\"\n    Compute the mean and element-wise variance of `h = a @ W^T + b` when the posterior has diagonal covariance.\n\n    Args:\n        a_mean (Tensor): Mean of the input `a`. Shape: `[B, D_in]`.\n        a_var (Tensor): Variance of the input `a`. Shape: `[B, D_in] `\n        weight (Tensor): Mean of the weights `W`. Shape: `[D_out, D_in]`\n        bias (Tensor): Mean of the bias `b`. Shape: `[D_out, ]`\n        b_var (Tensor): Element-wise variance of the bias `b`. Shape: `[D_out, ]`\n        w_var (Tensor): Element-wise variance of the weights `W`. Shape: `[D_out, D_in]`\n\n    Returns: \n        h_mean (Tensor): Mean of the pre-activations `h`. Shape: `[B, D_out]`\n        h_var (Tensor): Element-wise variance of the pre-activations `h`. Shape: `[B, D_out]`\n    \"\"\"\n\n    # calculate mean(h)\n    h_mean = F.linear(a_mean, weight, bias)\n\n    # calculate var(h)\n    weight_mean2_var_sum = weight ** 2 + w_var # [D_out, D_in]\n    h_var = a_mean **2 @ w_var.T + a_var @ weight_mean2_var_sum.T + b_var\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.forward_activation_implicit_diag","title":"<code>forward_activation_implicit_diag(activation_func, h_mean, h_var)</code>","text":"<p>Approximate the distribution of <code>a = g(h)</code> given <code>h ~ N(h_mean, h_var)</code>, where <code>h_var</code>  is the element-wise variance of pre-activation <code>h</code>. Uses a first-order Taylor expansion: <code>a ~ N(g(h_mean), g'(h_mean)^T @ h_var @ g'(h_mean))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>activation_func</code> <code>Callable</code> <p>A PyTorch activation function <code>g(\u00b7)</code> (e.g. <code>nn.ReLU()</code>)</p> required <code>h_mean</code> <code>Tensor</code> <p>Mean of the pre-activations <code>h</code>. Shape: <code>[B, D]</code></p> required <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the pre-activations <code>h</code>. Shape: <code>[B, D]</code></p> required <p>Returns:</p> Name Type Description <code>a_mean</code> <code>Tensor</code> <p>Mean of the activations <code>a</code>. Shape: <code>[B, D]</code></p> <code>a_var</code> <code>Tensor</code> <p>Element-wise variance of the activations <code>a</code>. Shape: <code>[B, D]</code></p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward_activation_implicit_diag(activation_func, h_mean, h_var):\n\n    \"\"\"\n    Approximate the distribution of `a = g(h)` given `h ~ N(h_mean, h_var)`, where `h_var` \n    is the element-wise variance of pre-activation `h`.\n    Uses a first-order Taylor expansion: `a ~ N(g(h_mean), g'(h_mean)^T @ h_var @ g'(h_mean))`.\n\n    Args:\n        activation_func (Callable): A PyTorch activation function `g(\u00b7)` (e.g. `nn.ReLU()`)\n        h_mean (Tensor): Mean of the pre-activations `h`. Shape: `[B, D]`\n        h_var (Tensor): Element-wise variance of the pre-activations `h`. Shape: `[B, D]`\n\n    Returns:\n        a_mean (Tensor): Mean of the activations `a`. Shape: `[B, D]`\n        a_var (Tensor): Element-wise variance of the activations `a`. Shape: `[B, D]`\n    \"\"\"\n\n    h_mean_grad = h_mean.detach().clone().requires_grad_()\n\n    a_mean = activation_func(h_mean_grad)\n    a_mean.retain_grad()\n    a_mean.backward(torch.ones_like(a_mean)) #[N, D]\n\n    nabla = h_mean_grad.grad #[N, D]\n    a_var = nabla ** 2 * h_var\n\n    return a_mean.detach(), a_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_mlp.forward_batch_norm_diag","title":"<code>forward_batch_norm_diag(h_var, bn_weight, bn_running_var, bn_eps)</code>","text":"<p>Compute the output variance when a distribution <code>h ~ N(h_mean, h_var)</code> is passed through a BatchNorm layer.</p> <p>Parameters:</p> Name Type Description Default <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the input <code>h</code>. Shape: <code>[B, D]</code>.</p> required <code>bn_weight</code> <code>Tensor</code> <p>Batch normalization scale factor (gamma). Shape: <code>[D,]</code>.</p> required <code>bn_running_var</code> <code>Tensor</code> <p>Running variance used in batch normalization. Shape: <code>[D,]</code>.</p> required <code>bn_eps</code> <code>float</code> <p>Small constant added to the denominator for numerical stability.</p> required <p>Returns:</p> Name Type Description <code>output_var</code> <code>Tensor</code> <p>Element-wise variance of the output after batch normalization. Shape: <code>[B, D]</code>.</p> Source code in <code>suq/diag_suq_mlp.py</code> <pre><code>def forward_batch_norm_diag(h_var, bn_weight, bn_running_var, bn_eps):\n\n    \"\"\"\n    Compute the output variance when a distribution `h ~ N(h_mean, h_var)`\n    is passed through a BatchNorm layer.\n\n    Args:\n        h_var (Tensor): Element-wise variance of the input `h`. Shape: `[B, D]`.\n        bn_weight (Tensor): Batch normalization scale factor (gamma). Shape: `[D,]`.\n        bn_running_var (Tensor): Running variance used in batch normalization. Shape: `[D,]`.\n        bn_eps (float): Small constant added to the denominator for numerical stability.\n\n    Returns:\n        output_var (Tensor): Element-wise variance of the output after batch normalization. Shape: `[B, D]`.\n    \"\"\"\n\n    scale_factor = (1 / (bn_running_var.reshape(1, -1) + bn_eps)) * bn_weight.reshape(1, -1) **2 # [B, D]\n    output_var = scale_factor * h_var # [B, D]\n\n    return output_var\n</code></pre>"},{"location":"reference/#transformer-functions-with-diagonal-covariance","title":"Transformer Functions with Diagonal Covariance","text":""},{"location":"reference/#suq.diag_suq_transformer-classes","title":"Classes","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_LayerNorm_Diag","title":"<code>SUQ_LayerNorm_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>LayerNorm module with uncertainty propagation under SUQ.</p> <p>Wraps <code>nn.LayerNorm</code> and propagates input variance analytically using running statistics. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>LayerNorm</code> <code>LayerNorm</code> <p>The original layer norm module to wrap</p> required Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_LayerNorm_Diag(nn.Module):\n    \"\"\"\n    LayerNorm module with uncertainty propagation under SUQ.\n\n    Wraps `nn.LayerNorm` and propagates input variance analytically using running statistics. See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        LayerNorm (nn.LayerNorm): The original layer norm module to wrap\n    \"\"\"\n\n    def __init__(self, LayerNorm):\n        super().__init__()\n\n        self.LayerNorm = LayerNorm\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ LayerNorm layer.\n\n        Args:\n            x_mean (Tensor): Input mean. Shape: [B, T, D]\n            x_var (Tensor): Input element-wise variance. Shape: [B, T, D]\n\n        Returns:\n            out_mean (Tensor): Output mean after layer normalization. Shape: [B, T, D]\n            out_var (Tensor): Output element-wise variance after layer normalization. Shape: [B, T, D]\n        \"\"\"\n\n        with torch.no_grad():\n\n            out_mean = self.LayerNorm.forward(x_mean)\n            out_var = forward_layer_norm_diag(x_mean, x_var, self.LayerNorm.weight, 1e-5)\n\n        return out_mean, out_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_LayerNorm_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_LayerNorm_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ LayerNorm layer.</p> <p>Parameters:</p> Name Type Description Default <code>x_mean</code> <code>Tensor</code> <p>Input mean. Shape: [B, T, D]</p> required <code>x_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape: [B, T, D]</p> required <p>Returns:</p> Name Type Description <code>out_mean</code> <code>Tensor</code> <p>Output mean after layer normalization. Shape: [B, T, D]</p> <code>out_var</code> <code>Tensor</code> <p>Output element-wise variance after layer normalization. Shape: [B, T, D]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ LayerNorm layer.\n\n    Args:\n        x_mean (Tensor): Input mean. Shape: [B, T, D]\n        x_var (Tensor): Input element-wise variance. Shape: [B, T, D]\n\n    Returns:\n        out_mean (Tensor): Output mean after layer normalization. Shape: [B, T, D]\n        out_var (Tensor): Output element-wise variance after layer normalization. Shape: [B, T, D]\n    \"\"\"\n\n    with torch.no_grad():\n\n        out_mean = self.LayerNorm.forward(x_mean)\n        out_var = forward_layer_norm_diag(x_mean, x_var, self.LayerNorm.weight, 1e-5)\n\n    return out_mean, out_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Classifier_Diag","title":"<code>SUQ_Classifier_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>Classifier head with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps a standard linear classifier and applies closed-form mean and variance propagation. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>Linear</code> <p>The final classification head</p> required <code>w_var</code> <code>Tensor</code> <p>Element-wise variance of weight. Shape: <code>[D_out, D_in]</code></p> required <code>b_var</code> <code>Tensor</code> <p>Element-wise variance of bias. Shape: <code>[D_out]</code></p> required Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_Classifier_Diag(nn.Module):\n    \"\"\"\n    Classifier head with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps a standard linear classifier and applies closed-form mean and variance propagation.\n    See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        classifier (nn.Linear): The final classification head\n        w_var (Tensor): Element-wise variance of weight. Shape: `[D_out, D_in]`\n        b_var (Tensor): Element-wise variance of bias. Shape: `[D_out]`\n    \"\"\"\n\n    def __init__(self, classifier, w_var, b_var):\n        super().__init__()\n\n        self.weight = classifier.weight\n        self.bias = classifier.bias\n        self.w_var = w_var.reshape(self.weight.shape)\n        self.b_var = b_var.reshape(self.bias.shape)\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ linear layer.\n\n        Args.\n            x_mean (Tensor): Input mean. Shape: `[B, D_in]`\n            x_var (Tensor): Input element-wise variance. Shape: `[B, D_in]`\n\n        Returns:\n            h_mean (Tensor): Output mean. Shape: `[B, D_out]`\n            h_var (Tensor): Output element-wise variance. Shape: `[B, D_out]`\n        \"\"\"\n        with torch.no_grad():\n            h_mean, h_var = forward_aW_diag(x_mean, x_var, self.weight.data, self.bias.data, self.w_var, self.b_var)\n        return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Classifier_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_Classifier_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ linear layer.</p> <p>Args.     x_mean (Tensor): Input mean. Shape: <code>[B, D_in]</code>     x_var (Tensor): Input element-wise variance. Shape: <code>[B, D_in]</code></p> <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Output mean. Shape: <code>[B, D_out]</code></p> <code>h_var</code> <code>Tensor</code> <p>Output element-wise variance. Shape: <code>[B, D_out]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ linear layer.\n\n    Args.\n        x_mean (Tensor): Input mean. Shape: `[B, D_in]`\n        x_var (Tensor): Input element-wise variance. Shape: `[B, D_in]`\n\n    Returns:\n        h_mean (Tensor): Output mean. Shape: `[B, D_out]`\n        h_var (Tensor): Output element-wise variance. Shape: `[B, D_out]`\n    \"\"\"\n    with torch.no_grad():\n        h_mean, h_var = forward_aW_diag(x_mean, x_var, self.weight.data, self.bias.data, self.w_var, self.b_var)\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_TransformerMLP_Diag","title":"<code>SUQ_TransformerMLP_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>MLP submodule of a transformer block with uncertainty propagation under SUQ.</p> <p>Supports both deterministic and Bayesian forward modes with closed-form variance propagation. Used internally in <code>SUQ_Transformer_Block_Diag</code>.</p> <p>Parameters:</p> Name Type Description Default <code>MLP</code> <code>Module</code> <p>Original MLP submodule</p> required <code>determinstic</code> <code>bool</code> <p>Whether to treat the MLP weights as deterministic</p> <code>True</code> <code>w_fc_var</code> <code>Tensor</code> <p>Variance of the first linear layer in MLP(if Bayesian)</p> <code>None</code> <code>w_proj_var</code> <code>Tensor</code> <p>Variance of the second linear layer in MLP (if Bayesian)</p> <code>None</code> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_TransformerMLP_Diag(nn.Module):\n    \"\"\"\n    MLP submodule of a transformer block with uncertainty propagation under SUQ.\n\n    Supports both deterministic and Bayesian forward modes with closed-form variance propagation.\n    Used internally in `SUQ_Transformer_Block_Diag`.\n\n    Args:\n        MLP (nn.Module): Original MLP submodule\n        determinstic (bool): Whether to treat the MLP weights as deterministic\n        w_fc_var (Tensor, optional): Variance of the first linear layer in MLP(if Bayesian)\n        w_proj_var (Tensor, optional): Variance of the second linear layer in MLP (if Bayesian)\n    \"\"\"\n\n    def __init__(self, MLP, determinstic = True, w_fc_var = None, w_proj_var = None):\n        super().__init__()\n\n        self.MLP = MLP\n        self.determinstic = determinstic\n        if not determinstic:\n            self.w_fc_var = w_fc_var.reshape(self.MLP.c_fc.weight.shape)\n            self.w_proj_var = w_proj_var.reshape(self.MLP.c_proj.weight.shape)\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ Transformer MLP layer.\n\n        Args:\n            x_mean (Tensor): Input mean. Shape [B, T, D]\n            x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n        Returns:\n            h_mean (Tensor): Output mean. Shape [B, T, D]\n            h_var (Tensor): Output element-wise variance. Shape [B, T, D]\n        \"\"\"\n\n        # first fc layer\n        with torch.no_grad():\n            if self.determinstic:\n                h_mean, h_var = forward_linear_diag_determinstic_weight(x_mean, x_var, self.MLP.c_fc.weight.data, self.MLP.c_fc.bias.data)\n            else:\n                h_mean, h_var = forward_linear_diag_Bayesian_weight(x_mean, x_var, self.MLP.c_fc.weight.data, self.w_fc_var, self.MLP.c_fc.bias.data)\n        # activation function\n        h_mean, h_var = forward_activation_diag(self.MLP.gelu, h_mean, h_var)\n        # second fc layer\n        with torch.no_grad():\n            if self.determinstic:\n                h_mean, h_var = forward_linear_diag_determinstic_weight(h_mean, h_var, self.MLP.c_proj.weight.data, self.MLP.c_proj.bias.data)\n            else:\n                h_mean, h_var = forward_linear_diag_Bayesian_weight(h_mean, h_var, self.MLP.c_proj.weight.data, self.w_proj_var, self.MLP.c_proj.bias.data)\n\n        return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_TransformerMLP_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_TransformerMLP_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ Transformer MLP layer.</p> <p>Parameters:</p> Name Type Description Default <code>x_mean</code> <code>Tensor</code> <p>Input mean. Shape [B, T, D]</p> required <code>x_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape [B, T, D]</p> required <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Output mean. Shape [B, T, D]</p> <code>h_var</code> <code>Tensor</code> <p>Output element-wise variance. Shape [B, T, D]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ Transformer MLP layer.\n\n    Args:\n        x_mean (Tensor): Input mean. Shape [B, T, D]\n        x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n    Returns:\n        h_mean (Tensor): Output mean. Shape [B, T, D]\n        h_var (Tensor): Output element-wise variance. Shape [B, T, D]\n    \"\"\"\n\n    # first fc layer\n    with torch.no_grad():\n        if self.determinstic:\n            h_mean, h_var = forward_linear_diag_determinstic_weight(x_mean, x_var, self.MLP.c_fc.weight.data, self.MLP.c_fc.bias.data)\n        else:\n            h_mean, h_var = forward_linear_diag_Bayesian_weight(x_mean, x_var, self.MLP.c_fc.weight.data, self.w_fc_var, self.MLP.c_fc.bias.data)\n    # activation function\n    h_mean, h_var = forward_activation_diag(self.MLP.gelu, h_mean, h_var)\n    # second fc layer\n    with torch.no_grad():\n        if self.determinstic:\n            h_mean, h_var = forward_linear_diag_determinstic_weight(h_mean, h_var, self.MLP.c_proj.weight.data, self.MLP.c_proj.bias.data)\n        else:\n            h_mean, h_var = forward_linear_diag_Bayesian_weight(h_mean, h_var, self.MLP.c_proj.weight.data, self.w_proj_var, self.MLP.c_proj.bias.data)\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Attention_Diag","title":"<code>SUQ_Attention_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-attention module with uncertainty propagation under SUQ.</p> <p>Supports deterministic and Bayesian value projections, with optional diagonal covariance assumptions. For details see SUQ paper section A.6 Used internally in <code>SUQ_Transformer_Block_Diag</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Attention</code> <code>Module</code> <p>The original attention module</p> required <code>determinstic</code> <code>bool</code> <p>Whether to treat value projections as deterministic</p> <code>True</code> <code>diag_cov</code> <code>bool</code> <p>If True, only compute the diagoanl covariance for value</p> <code>False</code> <code>W_v_var</code> <code>Tensor</code> <p>Posterior variance for value matrix (if Bayesian)</p> <code>None</code> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_Attention_Diag(nn.Module):\n    \"\"\"\n    Self-attention module with uncertainty propagation under SUQ.\n\n    Supports deterministic and Bayesian value projections, with optional diagonal covariance assumptions. For details see SUQ paper section A.6\n    Used internally in `SUQ_Transformer_Block_Diag`.\n\n    Args:\n        Attention (nn.Module): The original attention module\n        determinstic (bool): Whether to treat value projections as deterministic\n        diag_cov (bool): If True, only compute the diagoanl covariance for value\n        W_v_var (Tensor, optional): Posterior variance for value matrix (if Bayesian)\n    \"\"\"\n\n    def __init__(self, Attention, determinstic = True, diag_cov = False, W_v_var = None):\n        super().__init__()\n\n        self.Attention = Attention\n        self.determinstic = determinstic\n        self.diag_cov = diag_cov\n\n        if not self.determinstic:\n            self.W_v_var = W_v_var # [D * D]\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ Attention layer.\n\n        Args:\n            x_mean (Tensor): Input mean. Shape [B, T, D]\n            x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n        Returns:\n            output_mean (Tensor): Output mean. Shape [B, T, D]\n            output_var (Tensor): Output element-wise variance. Shape [B, T, D]\n        \"\"\"\n\n        with torch.no_grad():\n\n            output_mean, attention_score = self.Attention.forward(x_mean, True)\n\n            n_h = self.Attention.n_head\n            B, T, D = x_mean.size()\n            D_v = D // n_h\n\n            W_v = self.Attention.c_attn_v.weight.data\n            project_W = self.Attention.c_proj.weight.data\n\n            if self.determinstic:\n                v_cov = forward_value_cov_determinstic_W(W_v, x_var, n_h, D_v)\n            else:\n                v_cov = forward_value_cov_Bayesian_W(W_v, self.W_v_var.reshape(D, D), x_mean, x_var, n_h, D_v, self.diag_cov)\n\n            QKV_cov = forward_QKV_cov(attention_score, v_cov, self.diag_cov)\n            output_var = forward_fuse_multi_head_cov(QKV_cov, project_W, self.diag_cov)\n\n            return output_mean, output_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Attention_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_Attention_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ Attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>x_mean</code> <code>Tensor</code> <p>Input mean. Shape [B, T, D]</p> required <code>x_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape [B, T, D]</p> required <p>Returns:</p> Name Type Description <code>output_mean</code> <code>Tensor</code> <p>Output mean. Shape [B, T, D]</p> <code>output_var</code> <code>Tensor</code> <p>Output element-wise variance. Shape [B, T, D]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ Attention layer.\n\n    Args:\n        x_mean (Tensor): Input mean. Shape [B, T, D]\n        x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n    Returns:\n        output_mean (Tensor): Output mean. Shape [B, T, D]\n        output_var (Tensor): Output element-wise variance. Shape [B, T, D]\n    \"\"\"\n\n    with torch.no_grad():\n\n        output_mean, attention_score = self.Attention.forward(x_mean, True)\n\n        n_h = self.Attention.n_head\n        B, T, D = x_mean.size()\n        D_v = D // n_h\n\n        W_v = self.Attention.c_attn_v.weight.data\n        project_W = self.Attention.c_proj.weight.data\n\n        if self.determinstic:\n            v_cov = forward_value_cov_determinstic_W(W_v, x_var, n_h, D_v)\n        else:\n            v_cov = forward_value_cov_Bayesian_W(W_v, self.W_v_var.reshape(D, D), x_mean, x_var, n_h, D_v, self.diag_cov)\n\n        QKV_cov = forward_QKV_cov(attention_score, v_cov, self.diag_cov)\n        output_var = forward_fuse_multi_head_cov(QKV_cov, project_W, self.diag_cov)\n\n        return output_mean, output_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Transformer_Block_Diag","title":"<code>SUQ_Transformer_Block_Diag</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single transformer block with uncertainty propagation under SUQ.</p> <p>Wraps LayerNorm, attention, and MLP submodules with uncertainty-aware versions. Used in <code>SUQ_ViT_Diag</code> to form a full transformer stack.</p> <p>Parameters:</p> Name Type Description Default <code>MLP</code> <code>Module</code> <p>Original MLP submodule</p> required <code>Attention</code> <code>Module</code> <p>Original attention submodule</p> required <code>LN_1</code> <code>LayerNorm</code> <p>Pre-attention layer norm</p> required <code>LN_2</code> <code>LayerNorm</code> <p>Pre-MLP layer norm</p> required <code>MLP_determinstic</code> <code>bool</code> <p>Whether to treat MLP as deterministic</p> required <code>Attn_determinstic</code> <code>bool</code> <p>Whether to treat attention as deterministic</p> required <code>diag_cov</code> <code>bool</code> <p>If True, only compute the diagoanl covariance for value</p> <code>False</code> <code>w_fc_var</code> <code>Tensor or None</code> <p>Posterior variance of MLP input projection (if Bayesian)</p> <code>None</code> <code>w_proj_var</code> <code>Tensor or None</code> <p>Posterior variance of MLP output projection (if Bayesian)</p> <code>None</code> <code>W_v_var</code> <code>Tensor or None</code> <p>Posterior variance of value matrix (if Bayesian)</p> <code>None</code> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_Transformer_Block_Diag(nn.Module):\n    \"\"\"\n    Single transformer block with uncertainty propagation under SUQ.\n\n    Wraps LayerNorm, attention, and MLP submodules with uncertainty-aware versions.\n    Used in `SUQ_ViT_Diag` to form a full transformer stack.\n\n    Args:\n        MLP (nn.Module): Original MLP submodule\n        Attention (nn.Module): Original attention submodule\n        LN_1 (nn.LayerNorm): Pre-attention layer norm\n        LN_2 (nn.LayerNorm): Pre-MLP layer norm\n        MLP_determinstic (bool): Whether to treat MLP as deterministic\n        Attn_determinstic (bool): Whether to treat attention as deterministic\n        diag_cov (bool): If True, only compute the diagoanl covariance for value\n        w_fc_var (Tensor or None): Posterior variance of MLP input projection (if Bayesian)\n        w_proj_var (Tensor or None): Posterior variance of MLP output projection (if Bayesian)\n        W_v_var (Tensor or None): Posterior variance of value matrix (if Bayesian)\n    \"\"\"\n\n\n    def __init__(self, MLP, Attention, LN_1, LN_2, MLP_determinstic, Attn_determinstic, diag_cov = False, w_fc_var = None, w_proj_var = None, W_v_var = None):\n        super().__init__()\n\n        self.ln_1 = SUQ_LayerNorm_Diag(LN_1)\n        self.ln_2 = SUQ_LayerNorm_Diag(LN_2)\n        self.attn = SUQ_Attention_Diag(Attention, Attn_determinstic, diag_cov, W_v_var)\n        self.mlp = SUQ_TransformerMLP_Diag(MLP, MLP_determinstic, w_fc_var, w_proj_var)\n\n    def forward(self, x_mean, x_var):\n        \"\"\"\n        Forward pass with uncertainty propagation through a SUQ Transformer block.    \n\n        Args:\n            x_mean (Tensor): Input mean. Shape [B, T, D]\n            x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n        Returns:\n            h_mean (Tensor): Output mean. Shape [B, T, D]\n            h_var (Tensor): Output element-wise variance. Shape [B, T, D]\n        \"\"\"\n\n        h_mean, h_var = self.ln_1(x_mean, x_var)\n        h_mean, h_var = self.attn(h_mean, h_var)\n        h_mean = h_mean + x_mean\n        h_var = h_var + x_var\n\n        old_h_mean, old_h_var = h_mean, h_var\n\n        h_mean, h_var = self.ln_2(h_mean, h_var)\n        h_mean, h_var = self.mlp(h_mean, h_var)\n        h_mean = h_mean + old_h_mean\n        h_var = h_var + old_h_var\n\n        return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_Transformer_Block_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_Transformer_Block_Diag.forward","title":"<code>forward(x_mean, x_var)</code>","text":"<p>Forward pass with uncertainty propagation through a SUQ Transformer block.    </p> <p>Parameters:</p> Name Type Description Default <code>x_mean</code> <code>Tensor</code> <p>Input mean. Shape [B, T, D]</p> required <code>x_var</code> <code>Tensor</code> <p>Input element-wise variance. Shape [B, T, D]</p> required <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Output mean. Shape [B, T, D]</p> <code>h_var</code> <code>Tensor</code> <p>Output element-wise variance. Shape [B, T, D]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, x_mean, x_var):\n    \"\"\"\n    Forward pass with uncertainty propagation through a SUQ Transformer block.    \n\n    Args:\n        x_mean (Tensor): Input mean. Shape [B, T, D]\n        x_var (Tensor): Input element-wise variance. Shape [B, T, D]\n\n    Returns:\n        h_mean (Tensor): Output mean. Shape [B, T, D]\n        h_var (Tensor): Output element-wise variance. Shape [B, T, D]\n    \"\"\"\n\n    h_mean, h_var = self.ln_1(x_mean, x_var)\n    h_mean, h_var = self.attn(h_mean, h_var)\n    h_mean = h_mean + x_mean\n    h_var = h_var + x_var\n\n    old_h_mean, old_h_var = h_mean, h_var\n\n    h_mean, h_var = self.ln_2(h_mean, h_var)\n    h_mean, h_var = self.mlp(h_mean, h_var)\n    h_mean = h_mean + old_h_mean\n    h_var = h_var + old_h_var\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_ViT_Diag","title":"<code>SUQ_ViT_Diag</code>","text":"<p>               Bases: <code>SUQ_Base</code></p> <p>Vision Transformer model with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.</p> <p>Wraps a ViT architecture into a structured uncertainty-aware model by replacing parts of the network with SUQ-compatible blocks. Allows selective Bayesian treatment of MLP and attention modules within each transformer block.</p> <p>Currently supports classification only. See the SUQ paper for theoretical background and assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>ViT</code> <code>Module</code> <p>A Vision Transformer model structured like <code>examples/vit_model.py</code></p> required <code>posterior_variance</code> <code>Tensor</code> <p>Flattened posterior variance vector</p> required <code>MLP_determinstic</code> <code>bool</code> <p>Whether MLP submodules are treated as deterministic</p> required <code>Attn_determinstic</code> <code>bool</code> <p>Whether attention submodules are treated as deterministic</p> required <code>scale_init</code> <code>float</code> <p>Initial value for the scale factor</p> <code>1.0</code> <code>attention_diag_cov</code> <code>bool</code> <p>If True, only compute the diagoanl covariance for value</p> <code>False</code> <code>likelihood</code> <code>str</code> <p>Currently only support 'Classification'</p> <code>'clasification'</code> <code>num_det_blocks</code> <code>int</code> <p>Number of transformer blocks to leave deterministic (from the bottom up)</p> <code>10</code> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>class SUQ_ViT_Diag(SUQ_Base):\n    \"\"\"\n    Vision Transformer model with uncertainty propagation under SUQ, with a diagonal Gaussian posterior.\n\n    Wraps a ViT architecture into a structured uncertainty-aware model by replacing parts\n    of the network with SUQ-compatible blocks. Allows selective Bayesian treatment of MLP\n    and attention modules within each transformer block.\n\n    Currently supports classification only. See the SUQ paper for theoretical background and assumptions.\n\n    Args:\n        ViT (nn.Module): A Vision Transformer model structured like `examples/vit_model.py`\n        posterior_variance (Tensor): Flattened posterior variance vector\n        MLP_determinstic (bool): Whether MLP submodules are treated as deterministic\n        Attn_determinstic (bool): Whether attention submodules are treated as deterministic\n        scale_init (float, optional): Initial value for the scale factor\n        attention_diag_cov (bool): If True, only compute the diagoanl covariance for value\n        likelihood (str): Currently only support 'Classification'\n        num_det_blocks (int): Number of transformer blocks to leave deterministic (from the bottom up)\n    \"\"\"\n\n    def __init__(self, ViT, posterior_variance, MLP_determinstic, Attn_determinstic, scale_init = 1.0, attention_diag_cov = False, likelihood = 'clasification', num_det_blocks = 10):\n        super().__init__(likelihood, scale_init)\n\n        if likelihood not in ['classification']:\n            raise ValueError(f\"{likelihood} not supported for ViT\")\n\n\n        self.transformer = nn.ModuleDict(dict(\n            pte = ViT.transformer.pte,\n            h = nn.ModuleList(),\n            ln_f = SUQ_LayerNorm_Diag(ViT.transformer.ln_f)\n        ))\n\n        self.scale_factor = nn.Parameter(torch.Tensor([scale_init]))\n\n        num_param_c_fc = ViT.transformer.h[0].mlp.c_fc.weight.numel()\n        num_param_c_proj = ViT.transformer.h[0].mlp.c_proj.weight.numel()\n        num_param_value_matrix = ViT.transformer.h[0].attn.c_proj.weight.numel()\n\n        index = 0\n        for block_index in range(len(ViT.transformer.h)):\n\n            if block_index &lt; num_det_blocks:\n                self.transformer.h.append(ViT.transformer.h[block_index])\n            else:\n                if not MLP_determinstic:\n                    w_fc_var = posterior_variance[index: index + num_param_c_fc]\n                    index += num_param_c_fc\n                    w_proj_var = posterior_variance[index: index + num_param_c_proj]\n                    index += num_param_c_proj\n                    self.transformer.h.append(\n                        SUQ_Transformer_Block_Diag(ViT.transformer.h[block_index].mlp, \n                                                    ViT.transformer.h[block_index].attn, \n                                                    ViT.transformer.h[block_index].ln_1, \n                                                    ViT.transformer.h[block_index].ln_2, \n                                                    MLP_determinstic,\n                                                    Attn_determinstic,\n                                                    attention_diag_cov,\n                                                    w_fc_var, \n                                                    w_proj_var,\n                                                    None))\n\n                if not Attn_determinstic:\n                    w_v_var = posterior_variance[index : index + num_param_value_matrix]\n                    index += num_param_value_matrix\n                    self.transformer.h.append(\n                        SUQ_Transformer_Block_Diag(ViT.transformer.h[block_index].mlp, \n                                                    ViT.transformer.h[block_index].attn, \n                                                    ViT.transformer.h[block_index].ln_1, \n                                                    ViT.transformer.h[block_index].ln_2, \n                                                    MLP_determinstic,\n                                                    Attn_determinstic,\n                                                    attention_diag_cov,\n                                                    None, \n                                                    None,\n                                                    w_v_var))\n\n        num_param_classifier_weight = ViT.classifier.weight.numel()\n        self.classifier = SUQ_Classifier_Diag(ViT.classifier, posterior_variance[index: index + num_param_classifier_weight], posterior_variance[index + num_param_classifier_weight:])\n\n    def forward_latent(self, pixel_values, interpolate_pos_encoding = None):\n\n        \"\"\"\n        Compute the predictive mean and variance of the ViT's latent output before applying the final likelihood layer.\n\n        Traverses the full transformer stack with uncertainty propagation.\n\n        Args:\n            pixel_values (Tensor): Input image tensor, shape [B, C, H, W]\n            interpolate_pos_encoding (optional): Optional positional embedding interpolation\n\n        Returns:\n            x_mean (Tensor): Predicted latent mean at the [CLS] token, shape [B, D]\n            x_var (Tensor): Predicted latent variance at the [CLS] token, shape [B, D]\n        \"\"\"\n\n        device = pixel_values.device\n\n        x_mean = self.transformer.pte(\n            pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n        )\n\n        # pass through model\n        x_var = torch.zeros_like(x_mean, device = device)\n\n        for i, block in enumerate(self.transformer.h):\n\n            if isinstance(block, SUQ_Transformer_Block_Diag):\n\n                x_mean, x_var = block(x_mean, x_var)\n            else:\n                x_mean = block(x_mean)\n\n        x_mean, x_var = self.transformer.ln_f(x_mean, x_var)\n\n        x_mean, x_var = self.classifier(x_mean[:, 0, :], x_var[:, 0, :])\n        x_var = x_var / self.scale_factor.to(device)\n\n        return x_mean, x_var\n\n    def forward(self, pixel_values, interpolate_pos_encoding = None):\n        \"\"\"\n        Compute predictive class probabilities using a probit approximation.\n\n        Performs a full forward pass through the ViT with uncertainty propagation, and\n        produces softmax-normalized class probabilities for classification.\n\n        Args:\n            pixel_values (Tensor): Input image tensor, shape [B, C, H, W]\n            interpolate_pos_encoding (optional): Optional positional embedding interpolation\n\n        Returns:\n            Tensor: Predicted class probabilities, shape [B, num_classes]\n        \"\"\"\n\n        x_mean, x_var = self.forward_latent(pixel_values, interpolate_pos_encoding)\n        kappa = 1 / torch.sqrt(1. + np.pi / 8 * x_var)\n\n        return torch.softmax(kappa * x_mean, dim=-1)\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_ViT_Diag-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.SUQ_ViT_Diag.forward_latent","title":"<code>forward_latent(pixel_values, interpolate_pos_encoding=None)</code>","text":"<p>Compute the predictive mean and variance of the ViT's latent output before applying the final likelihood layer.</p> <p>Traverses the full transformer stack with uncertainty propagation.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input image tensor, shape [B, C, H, W]</p> required <code>interpolate_pos_encoding</code> <code>optional</code> <p>Optional positional embedding interpolation</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_mean</code> <code>Tensor</code> <p>Predicted latent mean at the [CLS] token, shape [B, D]</p> <code>x_var</code> <code>Tensor</code> <p>Predicted latent variance at the [CLS] token, shape [B, D]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_latent(self, pixel_values, interpolate_pos_encoding = None):\n\n    \"\"\"\n    Compute the predictive mean and variance of the ViT's latent output before applying the final likelihood layer.\n\n    Traverses the full transformer stack with uncertainty propagation.\n\n    Args:\n        pixel_values (Tensor): Input image tensor, shape [B, C, H, W]\n        interpolate_pos_encoding (optional): Optional positional embedding interpolation\n\n    Returns:\n        x_mean (Tensor): Predicted latent mean at the [CLS] token, shape [B, D]\n        x_var (Tensor): Predicted latent variance at the [CLS] token, shape [B, D]\n    \"\"\"\n\n    device = pixel_values.device\n\n    x_mean = self.transformer.pte(\n        pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n    )\n\n    # pass through model\n    x_var = torch.zeros_like(x_mean, device = device)\n\n    for i, block in enumerate(self.transformer.h):\n\n        if isinstance(block, SUQ_Transformer_Block_Diag):\n\n            x_mean, x_var = block(x_mean, x_var)\n        else:\n            x_mean = block(x_mean)\n\n    x_mean, x_var = self.transformer.ln_f(x_mean, x_var)\n\n    x_mean, x_var = self.classifier(x_mean[:, 0, :], x_var[:, 0, :])\n    x_var = x_var / self.scale_factor.to(device)\n\n    return x_mean, x_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.SUQ_ViT_Diag.forward","title":"<code>forward(pixel_values, interpolate_pos_encoding=None)</code>","text":"<p>Compute predictive class probabilities using a probit approximation.</p> <p>Performs a full forward pass through the ViT with uncertainty propagation, and produces softmax-normalized class probabilities for classification.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input image tensor, shape [B, C, H, W]</p> required <code>interpolate_pos_encoding</code> <code>optional</code> <p>Optional positional embedding interpolation</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Predicted class probabilities, shape [B, num_classes]</p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward(self, pixel_values, interpolate_pos_encoding = None):\n    \"\"\"\n    Compute predictive class probabilities using a probit approximation.\n\n    Performs a full forward pass through the ViT with uncertainty propagation, and\n    produces softmax-normalized class probabilities for classification.\n\n    Args:\n        pixel_values (Tensor): Input image tensor, shape [B, C, H, W]\n        interpolate_pos_encoding (optional): Optional positional embedding interpolation\n\n    Returns:\n        Tensor: Predicted class probabilities, shape [B, num_classes]\n    \"\"\"\n\n    x_mean, x_var = self.forward_latent(pixel_values, interpolate_pos_encoding)\n    kappa = 1 / torch.sqrt(1. + np.pi / 8 * x_var)\n\n    return torch.softmax(kappa * x_mean, dim=-1)\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer-functions","title":"Functions","text":""},{"location":"reference/#suq.diag_suq_transformer.forward_linear_diag_Bayesian_weight","title":"<code>forward_linear_diag_Bayesian_weight(e_mean, e_var, w_mean, w_var, bias=None)</code>","text":"<p>Compute the mean and element-wise variance of <code>h = e @ W^T + b</code> when <code>e ~ N(e_mean, e_var)</code> and <code>W ~ N(w_mean, w_var)</code></p> Note <ul> <li>We only make the weight Bayesian, bias is treated determinstically</li> <li>We always assume the input to next layer has diagonal covariance, so we only compute the variance over <code>h</code> here.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>e_mean</code> <code>Tensor</code> <p>Mean of the input embeddings <code>e</code>. Shape: <code>[B, T, D_in]</code></p> required <code>e_var</code> <code>Tensor</code> <p>Element-wise variance of the input embeddings <code>e</code>. Shape: <code>[B, T, D_in]</code></p> required <code>w_mean</code> <code>Tensor</code> <p>Mean of the weights <code>W</code>. Shape: <code>[D_out, D_in]</code></p> required <code>w_var</code> <code>Tensor</code> <p>Element-wise variance of the weights <code>W</code>. Shape: <code>[D_out, D_in]</code></p> required <code>bias</code> <code>Tensor</code> <p>Bias term <code>b</code>. Shape: <code>[D_out,]</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Mean of the output <code>h</code>. Shape: <code>[B, T, D_out]</code></p> <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the output <code>h</code>. Shape: <code>[B, T, D_out]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_linear_diag_Bayesian_weight(e_mean, e_var, w_mean, w_var, bias = None):\n    \"\"\"\n    Compute the mean and element-wise variance of `h = e @ W^T + b` when `e ~ N(e_mean, e_var)` and `W ~ N(w_mean, w_var)`\n\n    Note:\n        - We only make the weight Bayesian, bias is treated determinstically\n        - We always assume the input to next layer has diagonal covariance, so we only compute the variance over `h` here.\n\n    Args:\n        e_mean (Tensor): Mean of the input embeddings `e`. Shape: `[B, T, D_in]`\n        e_var (Tensor): Element-wise variance of the input embeddings `e`. Shape: `[B, T, D_in]`\n        w_mean (Tensor): Mean of the weights `W`. Shape: `[D_out, D_in]`\n        w_var (Tensor): Element-wise variance of the weights `W`. Shape: `[D_out, D_in]`\n        bias (Tensor, optional): Bias term `b`. Shape: `[D_out,]`\n\n    Returns:\n        h_mean (Tensor): Mean of the output `h`. Shape: `[B, T, D_out]`\n        h_var (Tensor): Element-wise variance of the output `h`. Shape: `[B, T, D_out]`\n    \"\"\"\n\n    # calculate mean(h)\n    h_mean = F.linear(e_mean, w_mean, bias)\n\n    # calculate var(h)\n    weight_mean2_var_sum = w_mean ** 2 + w_var # [D_out, D_in]\n    h_var = e_mean **2 @ w_var.T + e_var @ weight_mean2_var_sum.T\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_linear_diag_determinstic_weight","title":"<code>forward_linear_diag_determinstic_weight(e_mean, e_var, weight, bias=None)</code>","text":"<p>Compute the mean and element-wise variance of <code>h = e @ W^T + b</code> when <code>e ~ N(e_mean, e_var)</code>, <code>W</code> and <code>b</code> are both determinstic</p> Note <ul> <li>We always assume the input to next layer has diagonal covariance, so we only compute the variance over <code>h</code> here.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>e_mean</code> <code>Tensor</code> <p>Mean of the input embeddings <code>e</code>. Shape: <code>[B, T, D_in]</code>.</p> required <code>e_var</code> <code>Tensor</code> <p>Element-wise variance of the input embeddings <code>e</code>. Shape: <code>[B, T, D_in]</code>.</p> required <code>weight</code> <code>Tensor</code> <p>Weights <code>W</code>. Shape: <code>[D_out, D_in]</code>.</p> required <code>bias</code> <code>Tensor</code> <p>Bias term <code>b</code>. Shape: <code>[D_out,]</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>h_mean</code> <code>Tensor</code> <p>Mean of the output <code>h</code>. Shape: <code>[B, T, D_out]</code></p> <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the output <code>h</code>. Shape: <code>[B, T, D_out]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_linear_diag_determinstic_weight(e_mean, e_var, weight, bias = None):\n    \"\"\"\n    Compute the mean and element-wise variance of `h = e @ W^T + b` when `e ~ N(e_mean, e_var)`, `W` and `b` are both determinstic\n\n    Note:\n        - We always assume the input to next layer has diagonal covariance, so we only compute the variance over `h` here.\n\n    Args:\n        e_mean (Tensor): Mean of the input embeddings `e`. Shape: `[B, T, D_in]`.\n        e_var (Tensor): Element-wise variance of the input embeddings `e`. Shape: `[B, T, D_in]`.\n        weight (Tensor): Weights `W`. Shape: `[D_out, D_in]`.\n        bias (Tensor, optional): Bias term `b`. Shape: `[D_out,]`.\n\n    Returns:\n        h_mean (Tensor): Mean of the output `h`. Shape: `[B, T, D_out]`\n        h_var (Tensor): Element-wise variance of the output `h`. Shape: `[B, T, D_out]`\n    \"\"\"\n\n    h_mean = F.linear(e_mean, weight, bias)\n    h_var = F.linear(e_var, weight ** 2, None)\n\n    return h_mean, h_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_activation_diag","title":"<code>forward_activation_diag(activation_func, h_mean, h_var)</code>","text":"<p>Approximate the distribution of <code>a = g(h)</code> given <code>h ~ N(h_mean, h_var)</code>, where <code>h_var</code>  is the element-wise variance of pre-activation <code>h</code>. Uses a first-order Taylor expansion: <code>a ~ N(g(h_mean), g'(h_mean)^T @ h_var @ g'(h_mean))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>activation_func</code> <code>Callable</code> <p>A PyTorch activation function <code>g(\u00b7)</code> (e.g. <code>nn.ReLU()</code>)</p> required <code>h_mean</code> <code>Tensor</code> <p>Mean of the pre-activations <code>h</code>. Shape: <code>[B, T, D]</code></p> required <code>h_var</code> <code>Tensor</code> <p>Element-wise variance of the pre-activations <code>h</code>. Shape: <code>[B, T, D]</code></p> required <p>Returns:</p> Name Type Description <code>a_mean</code> <code>Tensor</code> <p>Mean of the activations <code>a</code>. Shape: <code>[B, T, D]</code></p> <code>a_var</code> <code>Tensor</code> <p>Element-wise variance of the activations <code>a</code>. Shape: <code>[B, T, D]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>@torch.enable_grad()\ndef forward_activation_diag(activation_func, h_mean, h_var):\n    \"\"\"\n    Approximate the distribution of `a = g(h)` given `h ~ N(h_mean, h_var)`, where `h_var` \n    is the element-wise variance of pre-activation `h`.\n    Uses a first-order Taylor expansion: `a ~ N(g(h_mean), g'(h_mean)^T @ h_var @ g'(h_mean))`.\n\n    Args:\n        activation_func (Callable): A PyTorch activation function `g(\u00b7)` (e.g. `nn.ReLU()`)\n        h_mean (Tensor): Mean of the pre-activations `h`. Shape: `[B, T, D]`\n        h_var (Tensor): Element-wise variance of the pre-activations `h`. Shape: `[B, T, D]`\n\n    Returns:\n        a_mean (Tensor): Mean of the activations `a`. Shape: `[B, T, D]`\n        a_var (Tensor): Element-wise variance of the activations `a`. Shape: `[B, T, D]`\n    \"\"\"\n\n    h_mean_grad = h_mean.detach().clone().requires_grad_()\n\n    a_mean = activation_func(h_mean_grad)\n    a_mean.retain_grad()\n    a_mean.backward(torch.ones_like(a_mean)) #[B, T, D]\n\n    nabla = h_mean_grad.grad #[B, T, D]\n    a_var = nabla ** 2 * h_var\n\n    return a_mean.detach(), a_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_layer_norm_diag","title":"<code>forward_layer_norm_diag(e_mean, e_var, ln_weight, ln_eps)</code>","text":"<p>Compute the output variance when a distribution <code>e ~ N(e_mean, e_var)</code> is passed through a LayerhNorm layer.</p> <p>Parameters:</p> Name Type Description Default <code>e_mean</code> <code>Tensor</code> <p>Mean of the input distribution. Shape: <code>[B, T, D]</code></p> required <code>e_var</code> <code>Tensor</code> <p>Element-wise variance of the input distribution. Shape: <code>[B, T, D]</code></p> required <code>ln_weight</code> <code>Tensor</code> <p>LayerNorm scale factor (gamma). Shape: <code>[D,]</code></p> required <code>ln_eps</code> <code>float</code> <p>Small constant added for numerical stability.</p> required <p>Returns:</p> Name Type Description <code>output_var</code> <code>Tensor</code> <p>Element-wise variance after LayerNorm. Shape: <code>[B, T, D]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_layer_norm_diag(e_mean, e_var, ln_weight, ln_eps):\n    \"\"\"\n    Compute the output variance when a distribution `e ~ N(e_mean, e_var)`\n    is passed through a LayerhNorm layer.\n\n    Args:\n        e_mean (Tensor): Mean of the input distribution. Shape: `[B, T, D]`\n        e_var (Tensor): Element-wise variance of the input distribution. Shape: `[B, T, D]`\n        ln_weight (Tensor): LayerNorm scale factor (gamma). Shape: `[D,]`\n        ln_eps (float): Small constant added for numerical stability.\n\n    Returns:\n        output_var (Tensor): Element-wise variance after LayerNorm. Shape: `[B, T, D]`\n    \"\"\"\n\n    # calculate the var\n    input_mean_var = e_mean.var(dim=-1, keepdim=True, unbiased=False) # [B, T, 1]\n    scale_factor = (1 / (input_mean_var + ln_eps)) * ln_weight **2 # [B, T, D]\n    output_var = scale_factor * e_var # [B, T, D]\n\n    return output_var\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_value_cov_Bayesian_W","title":"<code>forward_value_cov_Bayesian_W(W_v, W_v_var, e_mean, e_var, n_h, D_v, diag_cov=False)</code>","text":"<p>Given value weight W_v ~ N(W_v, W_v_var) and input E ~ N(e_mean, e_var) Compute the covariance of output <code>v = W_v @ E</code></p> <p>Parameters:</p> Name Type Description Default <code>n_h</code> <code>int</code> <p>Number of attention heads.</p> required <code>D_v</code> <code>int</code> <p>Dimension per head. Must satisfy <code>n_h * D_v = D</code></p> required <code>W_v</code> <code>Tensor</code> <p>Mean of the value weight <code>W_v</code>. Shape: <code>[D, D]</code></p> required <code>W_v_var</code> <code>Tensor</code> <p>Element-wise variance of the value weight <code>W_v</code>. Shape: <code>[D, D]</code></p> required <code>e_mean</code> <code>Tensor</code> <p>Mean of the input embeddings <code>e</code>. Shape: <code>[B, T, D]</code></p> required <code>e_var</code> <code>Tensor</code> <p>Element-wise variance of the input embeddings <code>e</code>. Shape: <code>[B, T, D]</code></p> required <code>diag_cov</code> <code>bool</code> <p>If <code>True</code>, only compute and return diagonal of the output covariance.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>v_var</code> <code>Tensor</code> <p>Returned if <code>diag_cov=True</code>. Element-wise variance of the output <code>v</code>.  Shape: <code>[B, T, n_h, D_v]</code></p> <code>v_cov</code> <code>Tensor</code> <p>Returned if <code>diag_cov=False</code>. Full covariance matrices of the output <code>v</code>.  Shape: <code>[B, T, n_h, D_v, D_v]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_value_cov_Bayesian_W(W_v, W_v_var, e_mean, e_var, n_h, D_v, diag_cov = False):\n    \"\"\"\n    Given value weight W_v ~ N(W_v, W_v_var) and input E ~ N(e_mean, e_var)\n    Compute the covariance of output `v = W_v @ E `\n\n    Args:\n        n_h (int): Number of attention heads.\n        D_v (int): Dimension per head. Must satisfy `n_h * D_v = D`\n        W_v (Tensor): Mean of the value weight `W_v`. Shape: `[D, D]`\n        W_v_var (Tensor): Element-wise variance of the value weight `W_v`. Shape: `[D, D]`\n        e_mean (Tensor): Mean of the input embeddings `e`. Shape: `[B, T, D]`\n        e_var (Tensor): Element-wise variance of the input embeddings `e`. Shape: `[B, T, D]`\n        diag_cov (bool): If `True`, only compute and return diagonal of the output covariance.\n\n    Returns:\n        v_var (Tensor): Returned if `diag_cov=True`. Element-wise variance of the output `v`.  Shape: `[B, T, n_h, D_v]`\n        v_cov (Tensor): Returned if `diag_cov=False`. Full covariance matrices of the output `v`.  Shape: `[B, T, n_h, D_v, D_v]`\n    \"\"\"\n\n    B, T, D = e_var.size()\n\n    if not diag_cov:\n        ## compute general covariance \n        W_v_reshaped = W_v.reshape(1, 1, n_h, D_v, D) \n            # [D, D] -&gt; [1, 1, n_h, D_v, D]\n        input_var_reshaped = e_var.reshape(B, T, 1, 1, D)\n            # [B, T, D] -&gt; [B, T, 1, 1, D]\n        v_cov = (W_v_reshaped * input_var_reshaped).transpose(3, 4)\n            # [1, 1, n_h, D_v, D] * [B, T, 1, 1, D] -&gt; [B, T, n_h, D_v, D] -&gt; [B, T, n_h, D, D_v]\n        v_cov = torch.matmul(W_v_reshaped, v_cov)\n            #  [1, 1, n_h, D_v, D] @ [B, T, n_h, D, D_v]  -&gt; [B, T, n_h, D_v, D_v]\n\n        ## add missing part for variance\n        W_v_var_reshaped = W_v_var.reshape(1, 1, n_h, D_v, D) \n            #[D, D] -&gt; [1, 1, n_h, D_v, D]\n        input_var_plus_mean_square = input_var_reshaped + e_mean.reshape(B, T, 1, 1, D)**2 #[B, T, 1, 1, D]\n        extra_var_term = torch.sum(input_var_plus_mean_square * W_v_var_reshaped, dim=[4]) # [B, T, n_h, D_v, D] -&gt; [B, T, n_h, D_v]\n        v_cov = v_cov + torch.diag_embed(extra_var_term) \n\n        return v_cov\n\n    else:\n        weight_mean2_var_sum = W_v **2 + W_v_var # [D, D]\n        v_var = e_mean **2 @ W_v_var.T + e_var @ weight_mean2_var_sum.T # [B, T, D]\n\n        return v_var.reshape(B, T, n_h, D_v)\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_value_cov_determinstic_W","title":"<code>forward_value_cov_determinstic_W(W_v, e_var, n_h, D_v, diag_cov=False)</code>","text":"<p>Given determinstic value weight W_v and input E ~ N(e_mean, e_var) Compute the covariance of output <code>v = W_v @ E</code></p> <p>Parameters:</p> Name Type Description Default <code>n_h</code> <code>int</code> <p>Number of attention heads.</p> required <code>D_v</code> <code>int</code> <p>Dimension per head. Must satisfy <code>n_h * D_v = D</code></p> required <code>W_v</code> <code>Tensor</code> <p>Value weight <code>W_v</code>. Shape: <code>[D, D]</code></p> required <code>e_var</code> <code>Tensor</code> <p>Element-wise variance of the input embeddings <code>e</code>. Shape: <code>[B, T, D]</code></p> required <code>diag_cov</code> <code>bool</code> <p>If <code>True</code>, only compute and return diagonal of the output covariance.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>v_var</code> <code>Tensor</code> <p>Returned if <code>diag_cov=True</code>. Element-wise variance of the output <code>v</code>.  Shape: <code>[B, T, n_h, D_v]</code></p> <code>v_cov</code> <code>Tensor</code> <p>Returned if <code>diag_cov=False</code>. Full covariance matrices of the output <code>v</code>.  Shape: <code>[B, T, n_h, D_v, D_v]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_value_cov_determinstic_W(W_v, e_var, n_h, D_v, diag_cov = False):\n    \"\"\"\n    Given determinstic value weight W_v and input E ~ N(e_mean, e_var)\n    Compute the covariance of output `v = W_v @ E`\n\n    Args:\n        n_h (int): Number of attention heads.\n        D_v (int): Dimension per head. Must satisfy `n_h * D_v = D`\n        W_v (Tensor): Value weight `W_v`. Shape: `[D, D]`\n        e_var (Tensor): Element-wise variance of the input embeddings `e`. Shape: `[B, T, D]`\n        diag_cov (bool): If `True`, only compute and return diagonal of the output covariance.\n\n    Returns:\n        v_var (Tensor): Returned if `diag_cov=True`. Element-wise variance of the output `v`.  Shape: `[B, T, n_h, D_v]`\n        v_cov (Tensor): Returned if `diag_cov=False`. Full covariance matrices of the output `v`.  Shape: `[B, T, n_h, D_v, D_v]`\n\n    \"\"\"\n\n    B, T, D = e_var.size()\n\n    if not diag_cov:\n        W_v_reshaped = W_v.reshape(1, 1, n_h, D_v, D) \n            #[n_h, D_v, D] -&gt; [1, 1, n_h, D_v, D]\n        input_var_reshaped = e_var.reshape(B, T, 1, 1, D)\n            # [B, T, D] -&gt; [B, T, 1, 1, D]\n        v_cov = (W_v_reshaped * input_var_reshaped).transpose(3, 4)\n            # [1, 1, n_h, D_v, D] * [B, T, 1, 1, D] -&gt; [B, T, n_h, D_v, D] -&gt; [B, T, n_h, D, D_v]\n        v_cov = torch.matmul(W_v_reshaped, v_cov)\n            #  [1, 1, n_h, D_v, D] @ [B, T, n_h, D, D_v]  -&gt; [B, T, n_h, D_v, D_v]\n\n        return v_cov\n\n    else:\n        v_var = e_var @ (W_v ** 2).T\n\n        return v_var.reshape(B, T, n_h, D_v)\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_QKV_cov","title":"<code>forward_QKV_cov(attention_score, v_cov, diag_cov=False)</code>","text":"<p>Given attention score (QK^T) and <code>V ~ N(v_mean, v_cov)</code> Compute the covariance of output <code>E = (QK^T) V</code></p> <p>Parameters:</p> Name Type Description Default <code>attention_score</code> <code>Tensor</code> <p>Attention weights <code>A = QK^T</code>. Shape: <code>[B, n_h, T, T]</code></p> required <code>v_cov</code> <code>Tensor</code> <p>Covariance of the value <code>V</code>.  Shape: <code>[B, T, n_h, D_v, D_v]</code> if <code>diag_cov=False</code>. <code>[B, T, n_h, D_v]</code> if <code>diag_cov=True</code></p> required <code>diag_cov</code> <code>bool</code> <p>If <code>True</code>, value <code>V</code> will have diagonal covariance</p> <code>False</code> <p>Returns:</p> Name Type Description <code>QKV_var</code> <code>Tensor</code> <p>Returned if <code>diag_cov=True</code>. Element-wise variance of the output <code>E</code>. Shape: <code>[B, T, n_h, D_v]</code></p> <code>QKV_cov</code> <code>Tensor</code> <p>Returned if <code>diag_cov=False</code>. Full covariance matrices of the output <code>E</code>. Shape: <code>[B, n_h, T, D_v, D_v]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_QKV_cov(attention_score, v_cov, diag_cov = False):\n    \"\"\"\n    Given attention score (QK^T) and `V ~ N(v_mean, v_cov)`\n    Compute the covariance of output `E = (QK^T) V`\n\n    Args:\n        attention_score (Tensor): Attention weights `A = QK^T`. Shape: `[B, n_h, T, T]`\n        v_cov (Tensor): Covariance of the value `V`.  Shape: `[B, T, n_h, D_v, D_v]` if `diag_cov=False`. `[B, T, n_h, D_v]` if `diag_cov=True`\n        diag_cov (bool): If `True`, value `V` will have diagonal covariance\n\n    Returns:\n        QKV_var (Tensor): Returned if `diag_cov=True`. Element-wise variance of the output `E`. Shape: `[B, T, n_h, D_v]`\n        QKV_cov (Tensor): Returned if `diag_cov=False`. Full covariance matrices of the output `E`. Shape: `[B, n_h, T, D_v, D_v]`\n    \"\"\"\n    if diag_cov:\n        B, T, n_h, D_v = v_cov.size()\n        QKV_cov = attention_score **2 @ v_cov.transpose(1, 2) # [B, n_h, T, D_v]\n            # v_cov [B, T, n_h, D_v] -&gt; [B, n_h, T, D_v]\n            # [B, n_h, T, T] @ [B, n_h, T, D_v]  -&gt; [B, n_h, T, D_v]\n    else:\n\n        B, T, n_h, D_v, _ = v_cov.size()\n\n        QKV_cov = attention_score **2 @ v_cov.permute(0, 2, 1, 3, 4).reshape(B, n_h, T, D_v * D_v) # [B, n_h, T, D_v * D_v]\n        # v_cov [B, T, n_h, D_v, D_v] -&gt; [B, n_h, T, D_v * D_v]\n        # [B, n_h, T, T] @ [B, n_h, T, D_v * D_v]  -&gt; [B, n_h, T, D_v * D_v]\n        QKV_cov = QKV_cov.reshape(B, n_h, T, D_v, D_v)\n\n    return QKV_cov\n</code></pre>"},{"location":"reference/#suq.diag_suq_transformer.forward_fuse_multi_head_cov","title":"<code>forward_fuse_multi_head_cov(QKV_cov, project_W, diag_cov=False)</code>","text":"<p>Given concatanated multi-head embedding <code>E ~ N(e_mean, e_cov)</code> and the determinstic projection weight matrix <code>W</code> Compute variance of each output dimenison </p> <p>Parameters:</p> Name Type Description Default <code>QKV_cov</code> <code>Tensor</code> <p>Covariance of the concatenated multi-head output <code>E</code>.  Shape: <code>[B, T, n_h, D_v, D_v]</code> if <code>diag_cov=False</code>. <code>[B, T, n_h, D_v]</code> if <code>diag_cov=True</code></p> required <code>project_W</code> <code>Tensor</code> <p>Projection weight matrix <code>W</code>. Shape: <code>[D_out, D_in]</code>, where <code>D_in = n_h * D_v</code></p> required <code>diag_cov</code> <code>bool</code> <p>If <code>True</code>, <code>QKV_cov</code> will have diagonal covariance</p> <code>False</code> <p>Returns:</p> Name Type Description <code>output_var</code> <code>Tensor</code> <p>Element-wise variance of the projected output. Shape: <code>[B, T, D_out]</code></p> Source code in <code>suq/diag_suq_transformer.py</code> <pre><code>def forward_fuse_multi_head_cov(QKV_cov, project_W, diag_cov = False):\n    \"\"\"\n    Given concatanated multi-head embedding `E ~ N(e_mean, e_cov)` and the determinstic projection weight matrix `W`\n    Compute variance of each output dimenison \n\n    Args:\n        QKV_cov (Tensor): Covariance of the concatenated multi-head output `E`.  Shape: `[B, T, n_h, D_v, D_v]` if `diag_cov=False`. `[B, T, n_h, D_v]` if `diag_cov=True`\n        project_W (Tensor): Projection weight matrix `W`. Shape: `[D_out, D_in]`, where `D_in = n_h * D_v`\n        diag_cov (bool): If `True`, `QKV_cov` will have diagonal covariance\n\n    Returns: \n        output_var (Tensor): Element-wise variance of the projected output. Shape: `[B, T, D_out]`\n    \"\"\"\n    if diag_cov:\n        B, n_h, T, D_v = QKV_cov.size()\n        output_var = QKV_cov.permute(0, 2, 1, 3).reshape(B, T, n_h * D_v) @ project_W.T ** 2\n            # QKV_cov [B, n_h, T, D_v] -&gt; [B, T, n_h, D_v] -&gt; [B, T, n_h * D_v]\n\n        return output_var\n\n    else:\n        B, n_h, T, D_v, _ = QKV_cov.size()\n        D, _ = project_W.shape\n\n        project_W_reshaped_1 = project_W.T.reshape(n_h, D_v, D).permute(0, 2, 1).reshape(n_h * D, D_v, 1)\n            # [n_h, D_v, D] -&gt; [n_h, D, D_v] -&gt; [n_h * D, D_v, 1]\n        project_W_reshaped_2 = project_W.T.reshape(n_h, D_v, D).permute(0, 2, 1).reshape(n_h * D, 1, D_v)\n            # [n_h, D_v, D] -&gt; [n_h, D, D_v] -&gt; [n_h * D, 1, D_v]\n\n        project_W_outer = torch.bmm(project_W_reshaped_1, project_W_reshaped_2).reshape(n_h, D, D_v, D_v).permute(1, 0, 2, 3) # [D, n_h, D_v, D_v]\n        # [n_h * D, D_v, D_v] @ [n_h * D, 1, D_v] -&gt; [n_h * D, D_v, D_v] -&gt; [D, n_h, D_v, D_v]\n\n        output_var_einsum = torch.einsum('dhij,bthij-&gt;dbt', project_W_outer, QKV_cov.permute(0, 2, 1, 3, 4))\n\n        return output_var_einsum.permute(1, 2, 0)\n</code></pre>"}]}